{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Text from Files, Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Text from a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/crimeandpunishment.txt\")\n",
    "rawtext = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Produced by John Bickers; and Dagny\\n\\nCRIME AND PUNISHMENT\\n\\nBy Fyodor Dostoevsky\\n\\n\\nTranslated By Cons'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawtext[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Get Concordance of the word \"pass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 42 matches:\n",
      "y time he went out he was obliged to pass her kitchen , the door of which inva\n",
      "at would it be if it somehow came to pass that I were really going to do it ? \n",
      "oom , she said , letting her visitor pass in front of her : '' Step in , my go\n",
      "d Katerina Ivanovna would not let it pass , she stood up for her ... and so th\n",
      "ng into the next room , as he had to pass through hers to get there . Taking n\n",
      "this scandal , and it came to such a pass that Dounia and I dared not even go \n",
      "for you . Oh , if only this comes to pass ! This would be such a benefit that \n",
      "irst place , because it will come to pass of itself , later on , and he will n\n",
      "hen the hour struck , it all came to pass quite differently , as it were accid\n",
      "g in the doorway not allowing him to pass , he advanced straight upon her . Sh\n",
      "im -- all was lost ; if they let him pass -- all was lost too ; they would rem\n",
      "t once that it would be loathsome to pass that seat on which after the girl wa\n",
      "h , but it 's nothing much , it will pass and you will be all right . Zossimov\n",
      " could you let things come to such a pass that she gave up sending you your di\n",
      "Yes , on my word ! Well , now let us pass to the United States of America , as\n",
      " , for all , and helping to bring to pass my neighbour 's getting a little mor\n",
      "azumihin got up this time to let him pass . Without glancing at anyone , and n\n",
      "o ! '' said Raskolnikov and tried to pass him . This was too much for Razumihi\n",
      " . For the family had come to such a pass that they were practically without c\n",
      "n his way home ; Raskolnikov let him pass , exchanging a silent greeting with \n",
      "y ashamed of it , and he hastened to pass to the other more practical cares an\n",
      " I ran away to take a flat he let it pass .... I put that in cleverly about a \n",
      "not live at all . I simply could n't pass by my mother starving , keeping my r\n",
      " in your eyes ... I can not let this pass considering the relationship and ...\n",
      "her almost to frenzy , and she would pass in an instant from the brightest hop\n"
     ]
    }
   ],
   "source": [
    "crimetokens = nltk.word_tokenize(rawtext)\n",
    "text = nltk.Text(crimetokens)\n",
    "text.concordance(\"pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Produced', 'by', 'John', 'Bickers', ';', 'and', 'Dagny', 'CRIME', 'AND', 'PUNISHMENT', 'By', 'Fyodor', 'Dostoevsky', 'Translated', 'By', 'Constance', 'Garnett', 'TRANSLATOR', \"'S\", 'PREFACE', 'A', 'few', 'words', 'about', 'Dostoevsky', 'himself', 'may', 'help', 'the', 'English', 'reader', 'to', 'understand', 'his', 'work', '.', 'Dostoevsky', 'was', 'the', 'son', 'of', 'a', 'doctor', '.', 'His', 'parents', 'were', 'very', 'hard-working', 'and', 'deeply', 'religious', 'people', ',', 'but', 'so', 'poor', 'that', 'they', 'lived', 'with', 'their', 'five', 'children', 'in', 'only', 'two', 'rooms', '.', 'The', 'father', 'and', 'mother', 'spent', 'their', 'evenings', 'in', 'reading', 'aloud', 'to', 'their', 'children', ',', 'generally', 'from', 'books', 'of', 'a', 'serious', 'character', '.', 'Though', 'always', 'sickly', 'and', 'delicate', 'Dostoevsky', 'came', 'out', 'third']\n",
      "\n",
      "There are 250325 tokens\n"
     ]
    }
   ],
   "source": [
    "# inspect the tokens\n",
    "print(crimetokens[:100])\n",
    "print(\"\\nThere are {:d} tokens\".format(len(crimetokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has two stemmers, Porter and Lancaster, described in section 3.6 of the NLTK book.  To use these stemmers, you first create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter stem words in text:\n",
      "\n",
      "['produc', 'by', 'john', 'bicker', ';', 'and', 'dagni', 'crime', 'and', 'punish', 'by', 'fyodor', 'dostoevski', 'translat', 'by', 'constanc', 'garnett', 'translat', \"'s\", 'prefac', 'a', 'few', 'word', 'about', 'dostoevski', 'himself', 'may', 'help', 'the', 'english', 'reader', 'to', 'understand', 'hi', 'work', '.', 'dostoevski', 'wa', 'the', 'son', 'of', 'a', 'doctor', '.', 'hi', 'parent', 'were', 'veri', 'hard-work', 'and', 'deepli', 'religi', 'peopl', ',', 'but', 'so', 'poor', 'that', 'they', 'live', 'with', 'their', 'five', 'children', 'in', 'onli', 'two', 'room', '.', 'the', 'father', 'and', 'mother', 'spent', 'their', 'even', 'in', 'read', 'aloud', 'to', 'their', 'children', ',', 'gener', 'from', 'book', 'of', 'a', 'seriou', 'charact', '.', 'though', 'alway', 'sickli', 'and', 'delic', 'dostoevski', 'came', 'out', 'third']\n",
      "\n",
      "Lancaster stem words in text:\n",
      "\n",
      "['produc', 'by', 'john', 'bick', ';', 'and', 'dagny', 'crim', 'and', 'pun', 'by', 'fyod', 'dostoevsky', 'transl', 'by', 'const', 'garnet', 'transl', \"'s\", 'prefac', 'a', 'few', 'word', 'about', 'dostoevsky', 'himself', 'may', 'help', 'the', 'engl', 'read', 'to', 'understand', 'his', 'work', '.', 'dostoevsky', 'was', 'the', 'son', 'of', 'a', 'doct', '.', 'his', 'par', 'wer', 'very', 'hard-working', 'and', 'deeply', 'religy', 'peopl', ',', 'but', 'so', 'poor', 'that', 'they', 'liv', 'with', 'their', 'fiv', 'childr', 'in', 'on', 'two', 'room', '.', 'the', 'fath', 'and', 'moth', 'spent', 'their', 'ev', 'in', 'read', 'aloud', 'to', 'their', 'childr', ',', 'gen', 'from', 'book', 'of', 'a', 'sery', 'charact', '.', 'though', 'alway', 'sick', 'and', 'del', 'dostoevsky', 'cam', 'out', 'third']\n"
     ]
    }
   ],
   "source": [
    "# stem all tokens in crime tokens\n",
    "crimePorterStem = [porter.stem(t) for t in crimetokens]\n",
    "print(\"Porter stem words in text:\\n\")\n",
    "print(crimePorterStem[:100])\n",
    "crimeLancStem = [lancaster.stem(t) for t in crimetokens]\n",
    "print(\"\\nLancaster stem words in text:\\n\")\n",
    "print(crimeLancStem[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Lancaster stemmer has lower-cased all the words, and in some cases, it appears to be a little more severe in removing word endings, but in others not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK has a lemmatizer that uses the WordNet on-line thesaurus as a dictionary to look up roots and find the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words in text:\n",
      "\n",
      "['Produced', 'by', 'John', 'Bickers', ';', 'and', 'Dagny', 'CRIME', 'AND', 'PUNISHMENT', 'By', 'Fyodor', 'Dostoevsky', 'Translated', 'By', 'Constance', 'Garnett', 'TRANSLATOR', \"'S\", 'PREFACE', 'A', 'few', 'words', 'about', 'Dostoevsky', 'himself', 'may', 'help', 'the', 'English', 'reader', 'to', 'understand', 'his', 'work', '.', 'Dostoevsky', 'was', 'the', 'son', 'of', 'a', 'doctor', '.', 'His', 'parents', 'were', 'very', 'hard-working', 'and', 'deeply', 'religious', 'people', ',', 'but', 'so', 'poor', 'that', 'they', 'lived', 'with', 'their', 'five', 'children', 'in', 'only', 'two', 'rooms', '.', 'The', 'father', 'and', 'mother', 'spent', 'their', 'evenings', 'in', 'reading', 'aloud', 'to', 'their', 'children', ',', 'generally', 'from', 'books', 'of', 'a', 'serious', 'character', '.', 'Though', 'always', 'sickly', 'and', 'delicate', 'Dostoevsky', 'came', 'out', 'third']\n",
      "Lemma words in text:\n",
      "\n",
      "['Produced', 'by', 'John', 'Bickers', ';', 'and', 'Dagny', 'CRIME', 'AND', 'PUNISHMENT', 'By', 'Fyodor', 'Dostoevsky', 'Translated', 'By', 'Constance', 'Garnett', 'TRANSLATOR', \"'S\", 'PREFACE', 'A', 'few', 'word', 'about', 'Dostoevsky', 'himself', 'may', 'help', 'the', 'English', 'reader', 'to', 'understand', 'his', 'work', '.', 'Dostoevsky', 'wa', 'the', 'son', 'of', 'a', 'doctor', '.', 'His', 'parent', 'were', 'very', 'hard-working', 'and', 'deeply', 'religious', 'people', ',', 'but', 'so', 'poor', 'that', 'they', 'lived', 'with', 'their', 'five', 'child', 'in', 'only', 'two', 'room', '.', 'The', 'father', 'and', 'mother', 'spent', 'their', 'evening', 'in', 'reading', 'aloud', 'to', 'their', 'child', ',', 'generally', 'from', 'book', 'of', 'a', 'serious', 'character', '.', 'Though', 'always', 'sickly', 'and', 'delicate', 'Dostoevsky', 'came', 'out', 'third']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "crimeLemma = [wnl.lemmatize(t) for t in crimetokens]\n",
    "print(\"Original words in text:\\n\")\n",
    "print(crimetokens[:100])\n",
    "print(\"Lemma words in text:\\n\")\n",
    "print(crimeLemma[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the WordNetLemmatizer does not stem verbs and in general, doesn’t stem very severely at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example: desert.txt\n",
    "* Tokenize the text\n",
    "* Apply and compare the Porter and Lancaster stemming method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1364 tokens\n"
     ]
    }
   ],
   "source": [
    "f = open(\"data/desert.txt\")\n",
    "desertRawText = f.read()\n",
    "f.close()\n",
    "desertTokens = nltk.word_tokenize(desertRawText)\n",
    "print(\"There are {:d} tokens\".format(len(desertTokens)))\n",
    "desertStemPorter = [porter.stem(t) for t in desertTokens]\n",
    "desertStemLancaster = [lancaster.stem(t) for t in desertTokens]\n",
    "desertLemma = [wnl.lemmatize(t) for t in desertTokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: understood | StemLancaster: understood | StemPorter: understood | Lemma: understood\n",
      "Original: to | StemLancaster: to | StemPorter: to | Lemma: to\n",
      "Original: the | StemLancaster: the | StemPorter: the | Lemma: the\n",
      "Original: set | StemLancaster: set | StemPorter: set | Lemma: set\n",
      "Original: the | StemLancaster: the | StemPorter: the | Lemma: the\n",
      "Original: stubborn | StemLancaster: stubborn | StemPorter: stubborn | Lemma: stubborn\n",
      "Original: finish | StemLancaster: fin | StemPorter: finish | Lemma: finish\n",
      "Original: 1,200 | StemLancaster: 1,200 | StemPorter: 1,200 | Lemma: 1,200\n",
      "Original: using | StemLancaster: us | StemPorter: use | Lemma: using\n",
      "Original: the | StemLancaster: the | StemPorter: the | Lemma: the\n"
     ]
    }
   ],
   "source": [
    "# compare porter and lancaster stem word at a particular index\n",
    "from random import sample\n",
    "\n",
    "i_list = sample(range(len(desertTokens)), 10)\n",
    "for i in i_list:\n",
    "    a = desertTokens[i]\n",
    "    b = desertStemLancaster[i]\n",
    "    c = desertStemPorter[i]\n",
    "    d = desertLemma[i]\n",
    "    print('Original: {:s} | StemLancaster: {:s} | StemPorter: {:s} | Lemma: {:s}'.format(a,b,c,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing a Text File End-to-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display first 50 words from file:\n",
      "['produced', 'by', 'john', 'bickers', ';', 'and', 'dagny', 'crime', 'and', 'punishment', 'by', 'fyodor', 'dostoevsky', 'translated', 'by', 'constance', 'garnett', 'translator', \"'s\", 'preface', 'a', 'few', 'words', 'about', 'dostoevsky', 'himself', 'may', 'help', 'the', 'english', 'reader', 'to', 'understand', 'his', 'work', '.', 'dostoevsky', 'was', 'the', 'son', 'of', 'a', 'doctor', '.', 'his', 'parents', 'were', 'very', 'hard-working', 'and']\n"
     ]
    }
   ],
   "source": [
    "# put the full path to the file here (or can use relative path from the directory of the program)\n",
    "#filepath = '/Users/njmccrac/NLPfall2016/labs/LabExamplesWeek4/CrimeAndPunishment.txt'\n",
    "#filepath = 'H:\\NLPclass\\LabExamplesWeek4\\CrimeAndPunishment.txt'\n",
    "filepath = 'data/CrimeAndPunishment.txt'\n",
    "\n",
    "def alpha_filter(w):\n",
    "  # pattern to match word of non-alphabetical characters\n",
    "  pattern = re.compile('^[^a-z]+$')\n",
    "  if (pattern.match(w)):\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "# open the file, read the text and close it\n",
    "f = open(filepath, 'r')\n",
    "filetext = f.read()\n",
    "f.close()\n",
    "\n",
    "# tokenize by the regular word tokenizer\n",
    "filetokens = nltk.word_tokenize(filetext)\n",
    "\n",
    "# choose to treat upper and lower case the same\n",
    "#    by putting all tokens in lower case\n",
    "filewords = [w.lower() for w in filetokens]\n",
    "\n",
    "# display the first words\n",
    "print (\"Display first 50 words from file:\")\n",
    "print (filewords[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display first 50 of 573 Stopwords:\n",
      "['â€™s', 'a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', 'aside', 'ask', 'asking']\n"
     ]
    }
   ],
   "source": [
    "# read a stop word file\n",
    "fstop = open('data/Smart.English.stop', 'r')\n",
    "stoptext = fstop.read()\n",
    "fstop.close()\n",
    "\n",
    "stopwords = nltk.word_tokenize(stoptext)\n",
    "print (\"Display first 50 of {:d} Stopwords:\".format(len(stopwords)))\n",
    "print (stopwords[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams from file with top 50 frequencies\n",
      "(('katerina', 'ivanovna'), 0.0008468990312593629)\n",
      "(('pyotr', 'petrovitch'), 0.000683111954459203)\n",
      "(('wo', \"n't\"), 0.0004913612304004793)\n",
      "(('ca', \"n't\"), 0.00048736642364925596)\n",
      "(('pulcheria', 'alexandrovna'), 0.00048337161689803257)\n",
      "(('avdotya', 'romanovna'), 0.0004594027763906921)\n",
      "(('rodion', 'romanovitch'), 0.0003435533806052132)\n",
      "(('porfiry', 'petrovitch'), 0.00032357934684909616)\n",
      "(('marfa', 'petrovna'), 0.00030760011984420254)\n",
      "(('sofya', 'semyonovna'), 0.0002836312793368621)\n",
      "(('raskolnikov', \"'s\"), 0.00021971437131728752)\n",
      "(('amalia', 'ivanovna'), 0.0002157195645660641)\n",
      "(('young', 'man'), 0.0002077299510636173)\n",
      "(('great', 'deal'), 0.00018775591730750026)\n",
      "((\"n't\", 'understand'), 0.00013981823629281934)\n",
      "(('ilya', 'petrovitch'), 0.0001318286227903725)\n",
      "(('ivanovna', \"'s\"), 0.0001238390092879257)\n",
      "(('sonia', \"'s\"), 0.00011584939578547888)\n",
      "(('make', 'haste'), 0.00010785978228303205)\n",
      "(('good', 'heavens'), 0.00010386497553180865)\n"
     ]
    }
   ],
   "source": [
    "# setup to process bigrams\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "       \n",
    "finder = BigramCollocationFinder.from_words(filewords)\n",
    "# choose to use both the non-alpha word filter and a stopwords filter\n",
    "finder.apply_word_filter(alpha_filter)\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "\n",
    "# score by frequency and display the top 50 bigrams\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "print (\"Bigrams from file with top 50 frequencies\")\n",
    "for item in scored[:20]:\n",
    "        print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigrams from file with top 50 mutual information scores\n",
      "(('praskovya', 'pavlovna'), 14.763517853413212)\n",
      "(('palais', 'de'), 14.34848035413437)\n",
      "(('de', 'cristal'), 14.348480354134367)\n",
      "(('explosive', 'lieutenant'), 14.248944680583456)\n",
      "(('semyon', 'zaharovitch'), 14.248944680583456)\n",
      "(('assistant', 'superintendent'), 13.91107504182707)\n",
      "(('arkady', 'ivanovitch'), 13.763517853413212)\n",
      "(('madame', 'resslich'), 13.567120640609708)\n",
      "(('afanasy', 'ivanovitch'), 13.34848035413437)\n",
      "(('nikodim', 'fomitch'), 13.34848035413437)\n",
      "(('andrey', 'semyonovitch'), 13.348480354134368)\n",
      "(('madame', 'lippevechsel'), 13.348480354134367)\n",
      "(('examining', 'lawyer'), 13.026552259247005)\n",
      "(('flushed', 'crimson'), 12.915520946858262)\n",
      "(('hay', 'market'), 12.91107504182707)\n",
      "(('chapter', 'iii'), 12.389122338631715)\n",
      "(('chapter', 'iv'), 12.389122338631715)\n",
      "(('dmitri', 'prokofitch'), 12.34848035413437)\n",
      "(('chapter', 'vi'), 12.348480354134367)\n",
      "(('canal', 'bank'), 12.322008142773177)\n"
     ]
    }
   ],
   "source": [
    "# score by PMI and display the top 50 bigrams\n",
    "# only use frequently occurring words in mutual information\n",
    "finder.apply_freq_filter(5)\n",
    "scored = finder.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "print (\"\\nBigrams from file with top 50 mutual information scores\")\n",
    "for item in scored[:20]:\n",
    "        print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This text is a Python string\n",
    "file0 = nltk.corpus.gutenberg.fileids()[0]\n",
    "emmatext = nltk.corpus.gutenberg.raw(file0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of this is: \n",
      " <class 'str'>\n",
      "The number of characters in the book is 887071\n",
      "\n",
      "The first 150 characters of the string:\n",
      "\n",
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to\n"
     ]
    }
   ],
   "source": [
    "print(\"The type of this is: \\n\", type(emmatext))\n",
    "print(\"The number of characters in the book is {:d}\\n\".format(len(emmatext)))\n",
    "print(\"The first 150 characters of the string:\\n\")\n",
    "print(emmatext[:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several newline characters. Replace them with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]  VOLUME I  CHAPTER I   Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to\n"
     ]
    }
   ],
   "source": [
    "newemmatext = emmatext.replace('\\n', ' ')\n",
    "print(newemmatext[:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of Regular Expressions for Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple text with no punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'book', 'is', 'interesting']\n"
     ]
    }
   ],
   "source": [
    "shorttext = \"That book is interesting\"\n",
    "pattern = re.compile(\"\\w+\") # find alphabetic characters\n",
    "print(re.findall(pattern, shorttext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text with some special characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U', 'S', 'A', 'poster', 'print', 'costs', '12', '40', 'but', 'with', '10', 'off']\n"
     ]
    }
   ],
   "source": [
    "specialtext = \"That U.S.A. poster-print costs $12.40, but with 10% off.\"\n",
    "print(re.findall(pattern, specialtext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting only alphabetic text leaves lots of the string unmatched.  Let’s start making a more general regular expression to match tokens by matching words that can have an internal hyphen.  In this case, we need to put parentheses around the part of the pattern that can be repeated 0 or more times.  Unfortunately, findall will then only report the part that matched inside those parentheses, so we’ll put an extra pair of parentheses around the whole match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('That', ''), ('U', ''), ('S', ''), ('A', ''), ('poster-print', '-print'), ('costs', ''), ('12', ''), ('40', ''), ('but', ''), ('with', ''), ('10', ''), ('off', '')]\n"
     ]
    }
   ],
   "source": [
    "ptoken = re.compile('(\\w+(-\\w+)*)')\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re.findall has reported both the whole matched text and the internal matched text, i.e. it reports the last match of any part of the regular expression in parentheses.  We could fix this by looking at the parts of the re.groups function to access only the outer match.  But let’s assume that we only want to look at outer matches and not at any of the internal matches.  We can instead make the internal parentheses into non-capturing subgroups.  This regular expression matches the same strings, but the findall function doesn’t report the subgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['end-of-line', 'character']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let’s check our pattern on a word with two internal hyphens.\n",
    "ptoken = re.compile('(\\w+(?:-\\w+)*)')\n",
    "re.findall(ptoken, 'end-of-line character')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to make a pattern to match abbreviations that might have a “.” inside, like U.S.A.  We only allow capitalized letters, and we make a simple pattern that matches alternating capital letters and dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('U.S.A.', 'A.')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pabbrev = re.compile('(([A-Z]\\.)+)')\n",
    "re.findall(pabbrev, specialtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This worked well, so let’s combine it with the words pattern to match either words or abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('That', '', ''), ('U', '', ''), ('S', '', ''), ('A', '', ''), ('poster-print', '-print', ''), ('costs', '', ''), ('12', '', ''), ('40', '', ''), ('but', '', ''), ('with', '', ''), ('10', '', ''), ('off', '', '')]\n"
     ]
    }
   ],
   "source": [
    "ptoken = re.compile('(\\w+(-\\w+)*|([A-Z]\\.)+)')\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that didn’t work because it first found the alphabetic words which found ‘U’, “S’ and “A’ as separate words before it could match the abbreviations.  So the order of the matching patterns really matters if an earlier pattern matches part of what you want to match.  We can switch the order of the token patterns to match abbreviations first and then alphabetics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('That', '', ''), ('U.S.A.', 'A.', ''), ('poster-print', '', '-print'), ('costs', '', ''), ('12', '', ''), ('40', '', ''), ('but', '', ''), ('with', '', ''), ('10', '', ''), ('off', '', '')]\n"
     ]
    }
   ],
   "source": [
    "ptoken = re.compile('(([A-Z]\\.)+|\\w+(-\\w+)*)')\n",
    "print(re.findall(ptoken, specialtext))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked much better.  Now we’ll add an expression to match the currency, with an optional $ so that we can also match numbers with optional decimal parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('U.S.A.', 'A.', '', ''), ('poster-print', '', '-print', ''), ('costs', '', '', ''), ('$12.40', '', '', '.40'), ('but', '', '', ''), ('with', '', '', ''), ('10', '', '', ''), ('off', '', '', '')]\n"
     ]
    }
   ],
   "source": [
    "ptoken = re.compile(' (([A-Z]\\.)+|\\w+(-\\w+)*|\\$?\\d+(\\.\\d+)?)')\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep on adding expressions, but the notation is getting awkward.  We can make a prettier regular expression that is equivalent to his one by using Python’s triple quotes (works for either “”” or ‘’’) that allows a string to go across multiple lines without adding a newline character.  We can use Python’s “r” before the string to get a “raw” string.  And we also use the regular expression verbose flag to allow us to put comments at the end of every line, which the re compiler will ignore.  But we seem to have to put extra parentheses around each of our disjunctions for the multi-line re to format correctly with findall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 'That', ''), ('U.S.A.', '', ''), ('', 'poster-print', ''), ('', 'costs', ''), ('', '', '$12.40'), ('', 'but', ''), ('', 'with', ''), ('', '10', ''), ('', 'off', '')]\n"
     ]
    }
   ],
   "source": [
    "p1 = '((?:[A-Z]\\.)+)' # abbreviations, e.g. U.S.A.\n",
    "p2 = '(\\w+(?:-\\w+)*)' # words with internal hyphens\n",
    "p3 = '(\\$?\\d+(?:\\.\\d+)?)' # currency, like $12.40\n",
    "p4 = '|'.join([p1,p2,p3]) # combine\n",
    "ptoken = re.compile(p4)\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about the function findall()\n",
    "\n",
    "Before we go on to finding tokens, we will look more at how to use the regular expression function findall().  Suppose that we have text with email addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_text = \"For more information, send a request to info@ischool.syr.edu. Or you can directly contact our information staff at HelpfulHenry@syr.edu and SageSue@syr.edu.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And suppose that we want to find the email addresses in two parts:  first the user name and then the domain.  We can define a regular expression with just the two inner parentheses to match these two parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: info, Domain:ischool.syr.edu.\n",
      "User: HelpfulHenry, Domain:syr.edu\n",
      "User: SageSue, Domain:syr.edu.\n"
     ]
    }
   ],
   "source": [
    "pemail = re.compile('([A-z]+)@([a-z.]+)')\n",
    "matches = re.findall(pemail, email_text)\n",
    "for m in matches:\n",
    "    # format function puts each argument into the output string where the {} is\n",
    "    email = 'User: {}, Domain:{}'.format(m[0],m[1])\n",
    "    print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expression Tokenizer using NLTK Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has built a tokenizing function that helps you write tokenizers by giving it the compiled pattern.  Regular expressions can also be written down in the “verbose” version, using the (?x) flag that allows the alternatives to be on different lines with comments, and it also alleviates the need to put extra parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize text1:\n",
      "\n",
      "['That', 'book', 'is', 'interesting']\n",
      "\n",
      "Tokenize text2:\n",
      "\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', ',', 'but', 'with', '10%', 'off', '.']\n"
     ]
    }
   ],
   "source": [
    "pattern = r''' (?x) \t\t# set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+    \t# abbreviations, e.g. U.S.A.\n",
    "        | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, $12.40, 50%\n",
    "        | \\w+(?:-\\w+)*  \t# words with internal hyphens\n",
    "        | \\.\\.\\.        \t# ellipsis\n",
    "        | [][.,;\"'?():-_%#']  # separate tokens\n",
    "        '''\n",
    "print(\"Tokenize text1:\\n\")\n",
    "print(nltk.regexp_tokenize(shorttext, pattern))\n",
    "print(\"\\nTokenize text2:\\n\")\n",
    "print(nltk.regexp_tokenize(specialtext, pattern))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might compare regular expression tokenizer with the built-in word tokenizer of NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U.S.A.', 'poster-print', 'costs', '$', '12.40', ',', 'but', 'with', '10', '%', 'off', '.']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.word_tokenize(specialtext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expression tokenizer appropriate for tweet text or other social media text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetPattern = r''' (?x)\t# set flag to allow verbose regexps\n",
    "      (?:https?://|www)\\S+    # simple URLs\n",
    "      | (?::-\\)|;-\\))\t\t# small list of emoticons\n",
    "      | &(?:amp|lt|gt|quot);  # XML or HTML entity\n",
    "      | \\#\\w+                 # hashtags\n",
    "      | @\\w+                  # mentions   \n",
    "      | \\d+:\\d+               # timelike pattern\n",
    "      | \\d+\\.\\d+              # number with a decimal\n",
    "      | (?:\\d+,)+?\\d{3}(?=(?:[^,]|$))   # number with a comma\n",
    "      | (?:[A-Z]\\.)+                    # simple abbreviations\n",
    "      | (?:--+)               # multiple dashes\n",
    "      | \\w+(?:-\\w+)*          # words with internal hyphens or apostrophes\n",
    "      | ['\\\".?!,:;/]+         # special characters\n",
    "      '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1 = \"@natalieohayre I agree #hc09 needs reform- but not by crooked politicians who r clueless about healthcare! #tcot #fishy NO GOV'T TAKEOVER!\"\n",
    "\n",
    "tweet2 = \"To Sen. Roland Burris: Affordable, quality health insurance can't wait http://bit.ly/j63je #hc09 #IL #60660\"\n",
    "\n",
    "tweet3 = \"RT @karoli: RT @Seriou: .@whitehouse I will stand w/ Obama on #healthcare,  I trust him. #p2 #tlot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization of tweet1 using custom regex pattern:\n",
      "\n",
      "['@natalieohayre', 'I', 'agree', '#hc09', 'needs', 'reform', 'but', 'not', 'by', 'crooked', 'politicians', 'who', 'r', 'clueless', 'about', 'healthcare', '!', '#tcot', '#fishy', 'NO', 'GOV', \"'\", 'T', 'TAKEOVER', '!']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenization of tweet1 using custom regex pattern:\\n\")\n",
    "print(nltk.regexp_tokenize(tweet1,tweetPattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization of tweet1 using built-in Tweet Tokenizer:\n",
      "\n",
      "['@natalieohayre', 'I', 'agree', '#hc09', 'needs', 'reform', '-', 'but', 'not', 'by', 'crooked', 'politicians', 'who', 'r', 'clueless', 'about', 'healthcare', '!', '#tcot', '#fishy', 'NO', \"GOV'T\", 'TAKEOVER', '!']\n"
     ]
    }
   ],
   "source": [
    "# using built-in tweet tokenizer in nltk\n",
    "tknzr = TweetTokenizer()\n",
    "print(\"Tokenization of tweet1 using built-in Tweet Tokenizer:\\n\")\n",
    "print(tknzr.tokenize(tweet1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Regex Pattern Added to Built-in regex tokenizer\n",
    "Run the regexp tokenizer with the regular pattern on the sentence “Mr. Black and Mrs. Brown attended the lecture by Dr. Gray, but Gov. White wasn’t there.”  \n",
    "* Design and add a line to the pattern of this tokenizer so that titles like “Mr.” are tokenized as having the dot inside the token.  Test and add some other titles to your list of titles.\n",
    "* Design and add the pattern of this tokenizer so that words with a single apostrophe, such as “wasn’t” are taken as a single token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My text is:\n",
      " Mr. Black and Mrs. Brown attended the lecture by Dr. Gray, but Gov. White wasn't there.\n"
     ]
    }
   ],
   "source": [
    "sentence = r'''Mr. Black and Mrs. Brown attended the lecture by Dr. Gray, but Gov. White wasn't there.'''\n",
    "print(\"My text is:\\n\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Black', 'and', 'Mrs.', 'Brown', 'attended', 'the', 'lecture', 'by', 'Dr.', 'Gray', 'but', 'Gov.', 'White', \"wasn't\", 'there']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile('|'.join([\n",
    "    \"(?:[A-Z][a-z]+\\.)+\", # abbreviations and titles, e.g. U.S.A., Gov., Dr., Mr., Mrs., etc...    \n",
    "    \"'?\\w[\\w']*(?:-\\w+)*'?\", # words with hyphens or '\n",
    "]))\n",
    "print(nltk.regexp_tokenize(sentence, pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "interpreter": {
   "hash": "2e67d2d51cc48135395e9785d6b518c9af6482520666ad00cf530c129701d75b"
  },
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit ('Continuum': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
