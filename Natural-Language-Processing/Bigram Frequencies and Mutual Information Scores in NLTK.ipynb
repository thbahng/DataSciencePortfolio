{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Lab Session Week 2\n",
    "# Bigram Frequencies and Mutual Information Scores in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "import re\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Now we want to set up the emma text again for processing by repeating steps that we did last week.  This gets the text of the book Emma, separates it into tokens with the word tokenizer, and converts all the characters to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text has 191776 words. This is a sample of 10 words:\n",
      "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter']\n"
     ]
    }
   ],
   "source": [
    "# get text of the book Emma\n",
    "file0 = nltk.corpus.gutenberg.fileids()[0]\n",
    "emmatext = nltk.corpus.gutenberg.raw(file0)\n",
    "# separate it into tokens\n",
    "emmatokens = nltk.word_tokenize(emmatext)\n",
    "# convert to lower case\n",
    "emmawords = [w.lower() for w in emmatokens]\n",
    "print(\"This text has {:d} words. This is a sample of 10 words:\".format(len(emmawords)))\n",
    "print(emmawords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we create a frequency distribution of the words, using the NLTK FreqDist module/class, and we show the 30 top frequency words.  Since the word frequency items are a pair of (word, frequency), we can use item 0 to get the word and item 1 to get the frequency.  Printing the string ‘\\t’ inserts a tab into the output, so that the frequency numbers line up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", \t 12016\n",
      ". \t 6346\n",
      "the \t 5201\n",
      "to \t 5181\n",
      "and \t 4877\n",
      "of \t 4284\n",
      "i \t 3177\n",
      "a \t 3124\n",
      "-- \t 3100\n",
      "it \t 2503\n"
     ]
    }
   ],
   "source": [
    "# frequency distribution of words from text\n",
    "ndist = FreqDist(emmawords)\n",
    "# 10 most common words\n",
    "nitems = ndist.most_common(10)\n",
    "for item in nitems:\n",
    "    print(item[0], '\\t', item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to get tokenized text\n",
    "emmawords2 = nltk.corpus.gutenberg.words(file0)\n",
    "emmawords2lowercase = [w.lower() for w in emmawords2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think the word lists that emmawords and emmawords2lowercase contain should be identical? Try this out and see if this is what you expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191776\n",
      "192427\n"
     ]
    }
   ],
   "source": [
    "print(len(emmawords))\n",
    "print(len(emmawords2lowercase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more words in emmawords2lowercase because it parses hyphenated words into multiple words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'she', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'s\", 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.', 'her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses', ';', 'and', 'her', 'place', 'had', 'been', 'supplied', 'by', 'an', 'excellent', 'woman', 'as', 'governess', ',', 'who', 'had', 'fallen', 'little', 'short', 'of', 'a', 'mother', 'in', 'affection', '.', 'sixteen', 'years', 'had', 'miss', 'taylor', 'been', 'in', 'mr.', 'woodhouse', \"'s\", 'family', ',', 'less', 'as', 'a', 'governess', 'than', 'a', 'friend', ',']\n"
     ]
    }
   ],
   "source": [
    "print(emmawords[:160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty', '-', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'she', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'\", 's', 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.', 'her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses', ';', 'and', 'her', 'place', 'had', 'been', 'supplied', 'by', 'an', 'excellent', 'woman', 'as', 'governess', ',', 'who', 'had', 'fallen', 'little', 'short', 'of', 'a', 'mother', 'in', 'affection', '.', 'sixteen', 'years', 'had', 'miss', 'taylor', 'been', 'in', 'mr', '.', 'woodhouse', \"'\", 's', 'family', ',', 'less', 'as', 'a']\n"
     ]
    }
   ],
   "source": [
    "print(emmawords2lowercase[:160])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering non-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using regular expressions\n",
    "# this regular expression pattern matches any word that contains all non-alphabetical\n",
    "#   lower-case characters [^a-z]+\n",
    "# the beginning ^ and ending $ require the match to begin and end on a word boundary \n",
    "pattern = re.compile('^[^a-z]+$')\n",
    "nonAlphaMatch = pattern.match('**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched non-alphabetical\n"
     ]
    }
   ],
   "source": [
    "#  if it matched, print a message\n",
    "if nonAlphaMatch: print ('matched non-alphabetical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a word and returns true if it consists only of non-alphabetic characters\n",
    "def alpha_filter(w):\n",
    "    # pattern to match a word of non-alphabetical characters\n",
    "    pattern = re.compile('^[^a-z]+$')\n",
    "    if pattern.match(w):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161456\n",
      "['emma', 'by', 'jane', 'austen', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', 'she', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', 'indulgent', 'father', 'and', 'had', 'in', 'consequence', 'of', 'her', 'sister', \"'s\", 'marriage', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', 'her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses']\n"
     ]
    }
   ],
   "source": [
    "alphaemmawords = [w for w in emmawords if not alpha_filter(w)] # filter for alphabetic words only\n",
    "print(len(alphaemmawords))\n",
    "print(alphaemmawords[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Stop Words\n",
    "\n",
    "### Stop words should be tailored to the tokenization and the analysis that you're trying to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "print(len(nltkstopwords))\n",
    "print(nltkstopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accommodate this tokenization, we will add some stopwords that have the apostrophe together with the contraction.  Here is the list that I developed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n",
      "[\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'could', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'might', 'mightn', \"mightn't\", 'more', 'most', 'must', 'mustn', \"mustn't\", 'my', 'myself', 'need', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'sha', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'wo', 'won', \"won't\", 'would', 'wouldn', \"wouldn't\", 'y', 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "morestopwords = ['could','would','might','must','need','sha','wo','y',\"'s\",\"'d\",\"'ll\",\"'t\",\"'m\",\"'re\",\"'ve\"]\n",
    "stopwords = nltkstopwords + morestopwords\n",
    "print(len(stopwords))\n",
    "print(sorted(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      " 161456\n",
      "After:\n",
      " 70579\n"
     ]
    }
   ],
   "source": [
    "# filtering using stopwords\n",
    "stoppedemmawords = [w for w in alphaemmawords if not w in stopwords]\n",
    "print(\"Before:\\n\", len(alphaemmawords))\n",
    "print(\"After:\\n\", len(stoppedemmawords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mr.', 1091), ('emma', 855), ('mrs.', 668), ('miss', 599), ('harriet', 496), ('much', 484), ('said', 483), ('one', 447), ('every', 435), ('weston', 430), ('thing', 392), ('think', 383), ('well', 378), ('elton', 378), ('knightley', 373), ('little', 359), ('never', 358), ('know', 335), ('good', 313), ('say', 310), ('woodhouse', 308), ('jane', 301), ('quite', 282), ('time', 275), ('great', 263), ('nothing', 252), ('dear', 241), ('always', 238), ('man', 232), ('fairfax', 232)]\n"
     ]
    }
   ],
   "source": [
    "# make frequency distribution with new filtered word list\n",
    "emmadist = FreqDist(stoppedemmawords)\n",
    "emmaitems = emmadist.most_common(30)\n",
    "print(emmaitems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the stop word list removes a lot of the non content-bearing words in the list, but many functional words remain as well as titles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'emma'), ('emma', 'by'), ('by', 'jane'), ('jane', 'austen'), ('austen', '1816'), ('1816', ']'), (']', 'volume'), ('volume', 'i'), ('i', 'chapter'), ('chapter', 'i'), ('i', 'emma'), ('emma', 'woodhouse'), ('woodhouse', ','), (',', 'handsome'), ('handsome', ','), (',', 'clever'), ('clever', ','), (',', 'and'), ('and', 'rich'), ('rich', ',')]\n"
     ]
    }
   ],
   "source": [
    "# get list of bigrams\n",
    "emmabigrams = list(nltk.bigrams(emmawords))\n",
    "print(emmabigrams[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we will obtain a lot more functionality by using the functions from the nltk.collocations package.  One of the places to obtain information about this package is in this section of the NLTK documentation called the HOWTOS.  We note that there are also Trigram functions in addition to the Bigram functions shown here.\n",
    "\n",
    "[http://www.nltk.org/howto/collocations.html](http://www.nltk.org/howto/collocations.html)\n",
    "\n",
    "To start using bigrams, we import the collocation finder module. \n",
    "from nltk.collocations import *\n",
    "\n",
    "Collocations are expressions of multiple words which commonly co-occur.\n",
    "\n",
    "Note that you must use the entire list of emmawords before any filtering or the raw bigrams may not be adjacent and will not be correct.  Start with all the words and then run the filters in the bigram finder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "The list is 63242 long\n",
      "<class 'tuple'> ((',', 'and'), 0.009813532454530285)\n"
     ]
    }
   ],
   "source": [
    "# variable for the bigram measures\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "# finder that allows us to call other functions to filter the bigrams collected and give scores to bigrams\n",
    "finder = BigramCollocationFinder.from_words(emmawords)\n",
    "# score bigrams by frequency; result is a list consisting of pairs, where each pair is a bigram and its score. The raw_freq measure returns frequency as the ratio of the count of the bigram over the count of total bigrams\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "print(type(scored))\n",
    "print(\"The list is {:d} long\".format(len(scored)))\n",
    "first = scored[0]\n",
    "print(type(first), first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((',', 'and'), 0.009813532454530285)\n",
      "(('.', \"''\"), 0.00603829467712331)\n",
      "((\"''\", '``'), 0.004995411313198732)\n",
      "((';', 'and'), 0.004520899382613049)\n",
      "(('to', 'be'), 0.0031547221758718505)\n",
      "((',', \"''\"), 0.0030452194226597696)\n",
      "(('.', 'i'), 0.0029722175871850494)\n",
      "((',', 'i'), 0.0029670031703654264)\n",
      "(('of', 'the'), 0.0029148590021691972)\n",
      "(('in', 'the'), 0.0023204154847321877)\n"
     ]
    }
   ],
   "source": [
    "# scores are sorted in order of decreasing frequency\n",
    "# The scores are the frequencies of the bigrams, normalized to fractions by the total number of bigrams.\n",
    "for bscore in scored[:10]:\n",
    "    print(bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any finder, we can also apply various filter functions. \n",
    "\n",
    "First let’s apply our alpha_filter that we created earlier.  It uses a filter that is applied to the individual words.  Note that the function apply_word_filter changes the bigram collocation in the variable “finder”, and any function which takes a word parameter and returns True or False as a result can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function alpha_filter in module __main__:\n",
      "\n",
      "alpha_filter(w)\n",
      "    # function that takes a word and returns true if it consists only of non-alphabetic characters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(alpha_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('to', 'be'), 0.0031547221758718505)\n",
      "(('of', 'the'), 0.0029148590021691972)\n",
      "(('in', 'the'), 0.0023204154847321877)\n",
      "(('it', 'was'), 0.0023047722342733187)\n",
      "(('i', 'am'), 0.00205448022693142)\n",
      "(('she', 'had'), 0.0017311863841148005)\n",
      "(('she', 'was'), 0.001710328716836309)\n",
      "(('had', 'been'), 0.0016008259636242283)\n",
      "(('it', 'is'), 0.0015538962122476222)\n",
      "(('i', 'have'), 0.0014652511263140331)\n"
     ]
    }
   ],
   "source": [
    "# filter out non-alphabetic words\n",
    "finder.apply_word_filter(alpha_filter)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:10]:\n",
    "    print(bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can filter out the stopwords if we wish.  Note that the lambda operates like a function definition “on-the-fly”, i.e. without a function name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('mr.', 'knightley'), 0.00142353579175705)\n",
      "(('mrs.', 'weston'), 0.0012827465376272318)\n",
      "(('mr.', 'elton'), 0.0011002419489404304)\n",
      "(('miss', 'woodhouse'), 0.0008864508593358918)\n",
      "(('mr.', 'weston'), 0.0008238778575004171)\n",
      "(('frank', 'churchill'), 0.0007508760220256966)\n",
      "(('mrs.', 'elton'), 0.0007300183547472051)\n",
      "(('mr.', 'woodhouse'), 0.000683088603370599)\n",
      "(('every', 'thing'), 0.0006465876856332388)\n",
      "(('miss', 'fairfax'), 0.000636158851993993)\n"
     ]
    }
   ],
   "source": [
    "# filter out stopwords\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:10]:\n",
    "    print(bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other filter methods (frequency threshold, word length)\n",
    "\n",
    "These are \"less meaningful\" filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((',', 'and'), 0.009813532454530285)\n",
      "(('.', \"''\"), 0.00603829467712331)\n",
      "((\"''\", '``'), 0.004995411313198732)\n",
      "((';', 'and'), 0.004520899382613049)\n",
      "(('to', 'be'), 0.0031547221758718505)\n",
      "((',', \"''\"), 0.0030452194226597696)\n",
      "(('.', 'i'), 0.0029722175871850494)\n",
      "((',', 'i'), 0.0029670031703654264)\n",
      "(('of', 'the'), 0.0029148590021691972)\n",
      "(('in', 'the'), 0.0023204154847321877)\n"
     ]
    }
   ],
   "source": [
    "# filter for words that occur with minimum frequency of 2\n",
    "finder2 = BigramCollocationFinder.from_words(emmawords)\n",
    "finder2.apply_freq_filter(2)\n",
    "scored = finder2.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:10]:\n",
    "    print(bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((\"''\", '``'), 0.004995411313198732)\n",
      "(('to', 'be'), 0.0031547221758718505)\n",
      "(('of', 'the'), 0.0029148590021691972)\n",
      "(('in', 'the'), 0.0023204154847321877)\n",
      "(('it', 'was'), 0.0023047722342733187)\n",
      "(('--', 'and'), 0.0017416152177540463)\n",
      "(('she', 'had'), 0.0017311863841148005)\n",
      "(('she', 'was'), 0.001710328716836309)\n",
      "(('had', 'been'), 0.0016008259636242283)\n",
      "(('it', 'is'), 0.0015538962122476222)\n",
      "(('could', 'not'), 0.0014496078758551643)\n",
      "(('mr.', 'knightley'), 0.00142353579175705)\n",
      "((\"''\", 'said'), 0.0013818204572000668)\n",
      "(('``', 'i'), 0.0013609627899215753)\n",
      "(('of', 'her'), 0.0013557483731019523)\n",
      "(('--', 'i'), 0.0013401051226430837)\n",
      "(('mrs.', 'weston'), 0.0012827465376272318)\n",
      "(('have', 'been'), 0.0012566744535291172)\n",
      "(('he', 'had'), 0.0012514600367094944)\n",
      "(('to', 'the'), 0.0012358167862506256)\n"
     ]
    }
   ],
   "source": [
    "# filter out bigrams where first word's length is less than 2\n",
    "finder2.apply_ngram_filter(lambda w1, w2: len(w1) < 2)\n",
    "scored = finder2.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:20]:\n",
    "    print(bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information and other scorers\n",
    "\n",
    "Recall that Mutual Information is a score introduced in the paper by Church and Hanks, where they defined it as an Association Ratio. Note that technically the original information theoretic definition of mutual information allows the two words to be in either order, but that the association ratio defined by Church and Hanks requires the words to be in order from left to right wherever they appear in the window\n",
    "\n",
    "In NLTK, the mutual information score is given by a function for Pointwise Mutual Information, where this is the version without the window.\n",
    "\n",
    "* Mutual information computes the probability of two words occurring in sequence.\n",
    "* The more strongly connected two items are, the higher their PMI score.\n",
    "* But when you apply the Mutual Information score to small documents or documents with rare words, the results don’t really make sense.  \n",
    "* It is recommended to run the PMI scorer with a minimum frequency of 5, which will make more sense on very large documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('d', \"'ye\"), 14.964100157849273)\n",
      "(('sore', 'throat'), 14.089631039933131)\n",
      "(('brunswick', 'square'), 13.952127516183198)\n",
      "(('william', 'larkins'), 13.089631039933133)\n",
      "(('baked', 'apples'), 12.964100157849273)\n",
      "(('box', 'hill'), 12.735994085318433)\n",
      "(('sixteen', 'miles'), 12.613602910765142)\n",
      "(('maple', 'grove'), 12.594866348183555)\n",
      "(('hair', 'cut'), 12.06363583140019)\n",
      "(('south', 'end'), 11.964100157849275)\n",
      "(('colonel', 'campbell'), 11.412166457515587)\n",
      "(('protest', 'against'), 11.34742879740078)\n",
      "(('robert', 'martin'), 11.093868032819602)\n",
      "(('five', 'couple'), 10.841703526489548)\n",
      "(('vast', 'deal'), 10.762466296679625)\n",
      "(('ready', 'wit'), 10.652225727625833)\n",
      "(('donwell', 'abbey'), 10.51931531517638)\n",
      "(('musical', 'society'), 10.509046979722552)\n",
      "(('infinitely', 'superior'), 10.230745817235448)\n",
      "(('married', 'women'), 10.057209562240756)\n"
     ]
    }
   ],
   "source": [
    "# no filtering, just scoring based on PMI (pointwise mutual information)\n",
    "# in order to use PMI, filter out words that have less than the minimum frequency of 5 (per Church and Hanks paper)\n",
    "finder3 = BigramCollocationFinder.from_words(emmawords)\n",
    "# filter on words with minimum frequency of 5\n",
    "finder3.apply_freq_filter(5)\n",
    "scored = finder3.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in scored[:20]:\n",
    "    print(bscore)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "interpreter": {
   "hash": "2e67d2d51cc48135395e9785d6b518c9af6482520666ad00cf530c129701d75b"
  },
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit ('Continuum': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
