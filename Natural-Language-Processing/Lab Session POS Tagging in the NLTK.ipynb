{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging in NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Tagged Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import nps_chat\n",
    "from nltk.corpus import brown\n",
    "from nltk import DefaultTagger\n",
    "from nltk import UnigramTagger\n",
    "from nltk import BigramTagger\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# print one tagged sentence from brown\n",
    "print(brown.tagged_sents()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.'), ('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "# print first 30 tagged words from brown\n",
    "print(brown.tagged_words()[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged word:\n",
      "\n",
      "('The', 'AT')\n"
     ]
    }
   ],
   "source": [
    "# here's a single tagged word\n",
    "wordtag = brown.tagged_words()[0]\n",
    "print(\"Tagged word:\\n\")\n",
    "print(wordtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:\n",
      "\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Type:\\n\")\n",
    "print(type(wordtag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element of tuple is the word:\n",
      "\n",
      "The\n",
      "\n",
      "Second element of tuple is the tag:\n",
      "\n",
      "AT\n"
     ]
    }
   ],
   "source": [
    "print(\"First element of tuple is the word:\\n\")\n",
    "print(wordtag[0])\n",
    "print(\"\\nSecond element of tuple is the tag:\\n\")\n",
    "print(wordtag[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown corpus is organized into different types of text, which can be selected by the categories argument, and it also allows you to map the tags to a simplified tag set, described in table 5.1 in the NLTK book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all the categories (text types) in brown corpus\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It', 'PRON'), ('was', 'VERB'), ('among', 'ADP'), ('these', 'DET'), ('that', 'ADP'), ('Hinkle', 'NOUN'), ('identified', 'VERB'), ('a', 'DET'), ('photograph', 'NOUN'), ('of', 'ADP'), ('Barco', 'NOUN'), ('!', '.'), ('!', '.'), ('For', 'ADP'), ('it', 'PRON'), ('seems', 'VERB'), ('that', 'ADP'), ('Barco', 'NOUN'), (',', '.'), ('fancying', 'VERB'), ('himself', 'PRON'), ('a', 'DET'), (\"ladies'\", 'NOUN'), ('man', 'NOUN'), ('(', '.'), ('and', 'CONJ'), ('why', 'ADV'), ('not', 'ADV'), (',', '.'), ('after', 'ADP'), ('seven', 'NUM'), ('marriages', 'NOUN'), ('?', '.'), ('?', '.'), (')', '.'), (',', '.'), ('had', 'VERB'), ('listed', 'VERB'), ('himself', 'PRON'), ('for', 'ADP'), ('Mormon', 'NOUN'), ('Beard', 'NOUN'), ('roles', 'NOUN'), ('at', 'ADP'), ('the', 'DET'), ('instigation', 'NOUN'), ('of', 'ADP'), ('his', 'DET'), ('fourth', 'ADJ'), ('murder', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "# text type humor, using universal tagset\n",
    "brown_humor_tagged = brown.tagged_words(categories='humor', tagset = 'universal')\n",
    "print(brown_humor_tagged[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other tagged corpora also come with the tagged_words method.  Note that the chat corpus is tagged with Penn Treebank POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('now', 'RB'), ('im', 'PRP'), ('left', 'VBD'), ('with', 'IN'), ('this', 'DT'), ('gay', 'JJ'), ('name', 'NN'), (':P', 'UH'), ('PART', 'VB'), ('hey', 'UH'), ('everyone', 'NN'), ('ah', 'UH'), ('well', 'UH'), ('NICK', 'NN'), (':', ':'), ('U7', 'NNP'), ('U7', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('gay', 'JJ'), ('name', 'NN'), ('.', '.'), ('.', 'SYM'), ('ACTION', 'NN'), ('gives', 'VBZ'), ('U121', 'NNP'), ('a', 'DT'), ('golf', 'NN'), ('clap', 'NN'), ('.', '.'), (':)', 'UH'), ('JOIN', 'VB'), ('hi', 'UH'), ('U59', 'NNP'), ('26', 'CD'), ('/', 'CC'), ('m', 'NN'), ('/', 'CC'), ('ky', 'NNP'), ('women', 'NNS'), ('that', 'WDT'), ('are', 'VBP'), ('nice', 'JJ'), ('please', 'VB'), ('pm', 'VB'), ('me', 'PRP'), ('JOIN', 'VB'), ('PART', 'VB'), ('there', 'RB'), ('ya', 'PRP')]\n"
     ]
    }
   ],
   "source": [
    "# nps_chat corpus uses Penn Treebank POS tags.\n",
    "from nltk.corpus import nps_chat\n",
    "print(nps_chat.tagged_words()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class, we will mostly use the Penn Treebank tag set, as it is the most widely used.  The Treebank has the tagged_words and tagged_sents methods, as well as the words method that we used before to get the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a string:\n",
      "\n",
      "( (S \n",
      "    (NP-SBJ \n",
      "      (NP (NNP Pierre) (NNP Vinken) )\n",
      "      (, ,) \n",
      "      (ADJP \n",
      "        (NP (CD 61) (NNS years) )\n",
      "        (JJ old) )\n",
      "      (, ,) ) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "# raw method\n",
    "treebank_text = treebank.raw()\n",
    "print(\"This is a string:\")\n",
    "print(treebank_text[:150],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.', 'Mr.', 'Vinken', 'is', 'chairman', 'of', 'Elsevier', 'N.V.', ',', 'the', 'Dutch', 'publishing', 'group', '.', 'Rudolph', 'Agnew', ',', '55', 'years', 'old', 'and', 'former', 'chairman', 'of', 'Consolidated', 'Gold', 'Fields', 'PLC', ',', 'was', 'named', '*-1', 'a']\n"
     ]
    }
   ],
   "source": [
    "# words (token) method\n",
    "treebank_tokens = treebank.words()\n",
    "print(treebank_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 tagged words in Penn TreeBank:\n",
      "\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.'), ('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.'), ('Rudolph', 'NNP'), ('Agnew', 'NNP'), (',', ','), ('55', 'CD'), ('years', 'NNS'), ('old', 'JJ'), ('and', 'CC'), ('former', 'JJ'), ('chairman', 'NN'), ('of', 'IN'), ('Consolidated', 'NNP'), ('Gold', 'NNP'), ('Fields', 'NNP'), ('PLC', 'NNP'), (',', ','), ('was', 'VBD'), ('named', 'VBN'), ('*-1', '-NONE-'), ('a', 'DT')]\n"
     ]
    }
   ],
   "source": [
    "treebank_tagged_words = treebank.tagged_words()[:50]\n",
    "print(\"First 50 tagged words in Penn TreeBank:\\n\")\n",
    "print(treebank_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 2 tagged sentences in Penn Treebank:\n",
      "\n",
      "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "treebank_tagged = treebank.tagged_sents()[:2]\n",
    "print(\"First 2 tagged sentences in Penn Treebank:\\n\")\n",
    "print(treebank_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK has almost 4,000 sentences of tagged data from Penn Treebank, while the actual Treebank has much more.  This will limit the accuracy of the POS taggers (and later parsers) that we can define in lab, but also make the running times short enough for labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags given on all tagged words in Penn Treebank:\n",
      "\n",
      "dict_keys(['NNP', ',', 'CD', 'NNS', 'JJ', 'MD', 'VB', 'DT', 'NN', 'IN', '.', 'VBZ', 'VBG', 'CC', 'VBD', 'VBN', '-NONE-', 'RB', 'TO', 'PRP', 'RBR', 'WDT', 'VBP', 'RP', 'PRP$', 'JJS', 'POS', '``', 'EX', \"''\", 'WP', ':', 'JJR', 'WRB', '$', 'NNPS', 'WP$', '-LRB-', '-RRB-', 'PDT', 'RBS', 'FW', 'UH', 'SYM', 'LS', '#']) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "tag_fd = FreqDist(tag for (word, tag) in treebank.tagged_words())\n",
    "print(\"Tags given on all tagged words in Penn Treebank:\\n\")\n",
    "print(tag_fd.keys(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Frequent Tags in Treebank:\n",
      "\n",
      "NN 13166\n",
      "IN 9857\n",
      "NNP 9410\n",
      "DT 8165\n",
      "-NONE- 6592\n",
      "NNS 6047\n",
      "JJ 5834\n",
      ", 4886\n",
      ". 3874\n",
      "CD 3546\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 Most Frequent Tags in Treebank:\\n\")\n",
    "for tag, freq in tag_fd.most_common(10):\n",
    "    print(tag, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Frequent Tag Classes in Treebank:\n",
      "\n",
      "N 28867\n",
      "V 12637\n",
      "I 9857\n",
      "D 8165\n",
      "- 6838\n",
      "J 6397\n",
      "C 5811\n",
      ", 4886\n",
      ". 3874\n",
      "P 3333\n"
     ]
    }
   ],
   "source": [
    "# use the first letter of the POS tag to get classes of tags\n",
    "tag_classes_fd = FreqDist(tag[0] for (word, tag) in treebank.tagged_words())\n",
    "# Top 10 tag classes\n",
    "print(\"Top 10 Most Frequent Tag Classes in Treebank:\\n\")\n",
    "for tag, freq in tag_classes_fd.most_common(10):\n",
    "    print(tag, freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger Training Setup\n",
    "* We will use the tagged sentences and words from the Penn Treebank\n",
    "* We separate our tagged data into a training set, where we'll learn the probabilities of the words and their tags, and a test set to evaluate how our taggers perform\n",
    "* This allows us to test the tagger’s accuracy on similar, but not the same, data that it was trained on\n",
    "* The training set is the first 90% of the sentences and the test set is the remaining 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "treebank_tagged = treebank.tagged_sents() # all tagged sentences of treebank\n",
    "size = int(len(treebank_tagged) * 0.9) # 90% for training\n",
    "treebank_train = treebank_tagged[:size] # training set\n",
    "treebank_test = treebank_tagged[size:] # test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the NLTK, a number of POS taggers are included in the tag module, including one that we can use that has been trained on all of Penn Treebank.  But for instructional purposes, we will develop a sequence of N-gram taggers whose performance improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a default tagger\n",
    "This default tagger just tags everything with the most frequent tag: NN. This simple tagger doesn't actually use the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NN'),\n",
       " ('Vinken', 'NN'),\n",
       " (',', 'NN'),\n",
       " ('61', 'NN'),\n",
       " ('years', 'NN'),\n",
       " ('old', 'NN'),\n",
       " (',', 'NN'),\n",
       " ('will', 'NN'),\n",
       " ('join', 'NN'),\n",
       " ('the', 'NN'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'NN'),\n",
       " ('a', 'NN'),\n",
       " ('nonexecutive', 'NN'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NN'),\n",
       " ('29', 'NN'),\n",
       " ('.', 'NN'),\n",
       " ('Mr.', 'NN'),\n",
       " ('Vinken', 'NN')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates the tagger\n",
    "from nltk import DefaultTagger\n",
    "t0 = DefaultTagger('NN')\n",
    "# show the effect of the tagger by tagging the first 20 words\n",
    "t0.tag(treebank_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK includes a function for taggers that computes tagging accuracy by comparing the result of a tagger with the original “gold standard” tagged text.  Here we use the NLTK function “evaluate” to apply the default tagger (to the untagged text) and compare it with the gold standard tagged text in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14697201017811704"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy of classifying all tokens in test set as 'NN' using default tagger\n",
    "t0.evaluate(treebank_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The evaluate function first takes the tagged text and removes the tags, so that only tokens are left.  \n",
    "* Then it runs the tagger, in this case t0, to tag all the text.  \n",
    "* Then it compares the tags predicted by the tagger with the “gold standard” tags already given.  \n",
    "* It reports the accuracy, which is the percentage of words with correct tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other simple taggers described in the NLTK book are the Regular Expression Tagger and the Lookup Tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Unigram Tagger\n",
    "* It tags each word with the most frequent tag that word has in the corpus.\n",
    "* For example, if the word “bank” occurs 30 times with the tag “NN” and 10 times with the tag “VB”, we’ll just tag it with “NN”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.'),\n",
       " ('Mr.', 'NNP'),\n",
       " ('Vinken', 'NNP')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import UnigramTagger\n",
    "t1 = UnigramTagger(treebank_tagged) # unigram tagger trained on entire tagged corpus\n",
    "# show the effect of the tagger by tagging the first 20 words\n",
    "t1.tag(treebank_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the tagger on the training set and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8627989821882952"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = UnigramTagger(treebank_train) # unigram tagger trained on training set only\n",
    "t1.evaluate(treebank_test) # evaluate on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Bigram Tagger with backoff to Unigram and Default Tagger\n",
    "Finally, NLTK has a Bigram tagger that can be trained using 2 tag-word sequences. \n",
    "But there will be unknown frequencies in the test data for the bigram tagger, and unknown words for the unigram tagger, so we can use the backoff tagger capability of NLTK to create a combined tagger.  This tagger uses bigram frequencies to tag as much as possible.  If a word doesn’t occur in a bigram, it uses the unigram tagger to tag that word.  If the word is unknown to the unigram tagger, then we use the default tagger to tag it as ‘NN’.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8905852417302799"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import BigramTagger\n",
    "t0 = DefaultTagger(\"NN\") # Default, tag words as 'NN'\n",
    "t1 = UnigramTagger(treebank_train, backoff=t0) # unigram tagger; backoff to default tagger\n",
    "t2 = BigramTagger(treebank_train, backoff=t1) # bigram tagger; backoff to unigram tagger\n",
    "t2.evaluate(treebank_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying N-Gram Tagger to Text Data\n",
    "* This function is actually trained to tokenize individual sentences and will work better if we first use the sentence splitter, aka tokenizer, to produce a list of text strings for individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Three Calgarians have found a rather unusual way of leaving snow and ice behind. They set off this week on foot and by camels on a grueling trek across the burning Arabian desert.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Three Calgarians have found a rather unusual way of leaving snow and ice behind.', 'They set off this week on foot and by camels on a grueling trek across the burning Arabian desert.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "textsplit = sent_tokenize(text)\n",
    "print(textsplit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After producing the list of sentence texts, apply the word tokenizer to each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Three', 'Calgarians', 'have', 'found', 'a', 'rather', 'unusual', 'way', 'of', 'leaving', 'snow', 'and', 'ice', 'behind', '.'], ['They', 'set', 'off', 'this', 'week', 'on', 'foot', 'and', 'by', 'camels', 'on', 'a', 'grueling', 'trek', 'across', 'the', 'burning', 'Arabian', 'desert', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokentext = [word_tokenize(sent) for sent in textsplit]\n",
    "print(tokentext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the t2 bigram POS tagger to each sentence of tokens in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Three', 'CD'), ('Calgarians', 'NN'), ('have', 'VBP'), ('found', 'VBN'), ('a', 'DT'), ('rather', 'RB'), ('unusual', 'JJ'), ('way', 'NN'), ('of', 'IN'), ('leaving', 'VBG'), ('snow', 'NN'), ('and', 'CC'), ('ice', 'NN'), ('behind', 'IN'), ('.', '.')], [('They', 'PRP'), ('set', 'VBN'), ('off', 'RP'), ('this', 'DT'), ('week', 'NN'), ('on', 'IN'), ('foot', 'NN'), ('and', 'CC'), ('by', 'IN'), ('camels', 'NN'), ('on', 'IN'), ('a', 'DT'), ('grueling', 'NN'), ('trek', 'NN'), ('across', 'IN'), ('the', 'DT'), ('burning', 'NN'), ('Arabian', 'NN'), ('desert', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "taggedtext = [t2.tag(tokens) for tokens in tokentext]\n",
    "print(taggedtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that this text has quite a few words that appear to be unknown to this tagger from the data it was trained on.  Examples of this are “Calgarians” and “camels”. In both cases, these two words are tagged as NN instead of the correct tags of NNPS and NNS, respectively.  This points out the benefit of adding sequence information such as an HMM tagger would use and lexical information, such as a Maximum Entropy tagger could use if you defined such features.  In the NLTK, another strategy would be to use a Regular Expression tagger as a backoff tagger that could take into account word features.\n",
    "\n",
    "NLTK includes the Stanford POS tagger already, which is available in the module 'taggers/maxent_treebank_pos_tagger/english.pickle' and it is used for the standard nltk.pos_tag function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Three', 'CD'), ('Calgarians', 'NNPS'), ('have', 'VBP'), ('found', 'VBN'), ('a', 'DT'), ('rather', 'RB'), ('unusual', 'JJ'), ('way', 'NN'), ('of', 'IN'), ('leaving', 'VBG'), ('snow', 'NN'), ('and', 'CC'), ('ice', 'NN'), ('behind', 'NN'), ('.', '.')], [('They', 'PRP'), ('set', 'VBD'), ('off', 'RP'), ('this', 'DT'), ('week', 'NN'), ('on', 'IN'), ('foot', 'NN'), ('and', 'CC'), ('by', 'IN'), ('camels', 'NNS'), ('on', 'IN'), ('a', 'DT'), ('grueling', 'NN'), ('trek', 'NN'), ('across', 'IN'), ('the', 'DT'), ('burning', 'NN'), ('Arabian', 'JJ'), ('desert', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# use the standard nltk POS tagger on the same example text\n",
    "from nltk import pos_tag\n",
    "taggedtextStanford = [pos_tag(tokens) for tokens in tokentext]\n",
    "print(taggedtextStanford)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Three', 'CD'), ('Calgarians', 'NN'), ('have', 'VBP'), ('found', 'VBN'), ('a', 'DT'), ('rather', 'RB'), ('unusual', 'JJ'), ('way', 'NN'), ('of', 'IN'), ('leaving', 'VBG'), ('snow', 'NN'), ('and', 'CC'), ('ice', 'NN'), ('behind', 'IN'), ('.', '.'), ('They', 'PRP'), ('set', 'VBN'), ('off', 'RP'), ('this', 'DT'), ('week', 'NN'), ('on', 'IN'), ('foot', 'NN'), ('and', 'CC'), ('by', 'IN'), ('camels', 'NN'), ('on', 'IN'), ('a', 'DT'), ('grueling', 'NN'), ('trek', 'NN'), ('across', 'IN'), ('the', 'DT'), ('burning', 'NN'), ('Arabian', 'NN'), ('desert', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# We can use a list comprehension to define the new list as all of the tagged tokens in each for the sentences\n",
    "taggedtext_flat = [pair for sent in taggedtext for pair in sent]\n",
    "print(taggedtext_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Three', 'CD'), ('Calgarians', 'NNPS'), ('have', 'VBP'), ('found', 'VBN'), ('a', 'DT'), ('rather', 'RB'), ('unusual', 'JJ'), ('way', 'NN'), ('of', 'IN'), ('leaving', 'VBG'), ('snow', 'NN'), ('and', 'CC'), ('ice', 'NN'), ('behind', 'NN'), ('.', '.'), ('They', 'PRP'), ('set', 'VBD'), ('off', 'RP'), ('this', 'DT'), ('week', 'NN'), ('on', 'IN'), ('foot', 'NN'), ('and', 'CC'), ('by', 'IN'), ('camels', 'NNS'), ('on', 'IN'), ('a', 'DT'), ('grueling', 'NN'), ('trek', 'NN'), ('across', 'IN'), ('the', 'DT'), ('burning', 'NN'), ('Arabian', 'JJ'), ('desert', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "taggedtextStanford_flat = [pair for sent in taggedtextStanford for pair in sent]\n",
    "print(taggedtextStanford_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a POS Tagger on a Large Text and Look at Tag Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from the book Emma\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import FreqDist\n",
    "\n",
    "file0 = gutenberg.fileids()[0]\n",
    "emmatext = gutenberg.raw(file0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process text\n",
    "sentlist = sent_tokenize(emmatext) # split raw text into sentences\n",
    "sentlisttokens = [word_tokenize(sent) for sent in sentlist] # tokenize the list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sentlisttokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('[', 'NNS'), ('Emma', 'NNP'), ('by', 'IN'), ('Jane', 'NNP'), ('Austen', 'NNP'), ('1816', 'CD'), (']', 'NNP'), ('VOLUME', 'NNP'), ('I', 'PRP'), ('CHAPTER', 'VBP'), ('I', 'PRP'), ('Emma', 'NNP'), ('Woodhouse', 'NNP'), (',', ','), ('handsome', 'NN'), (',', ','), ('clever', 'NN'), (',', ','), ('and', 'CC'), ('rich', 'JJ'), (',', ','), ('with', 'IN'), ('a', 'DT'), ('comfortable', 'JJ'), ('home', 'NN'), ('and', 'CC'), ('happy', 'JJ'), ('disposition', 'NN'), (',', ','), ('seemed', 'VBD'), ('to', 'TO'), ('unite', 'VB'), ('some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('blessings', 'NNS'), ('of', 'IN'), ('existence', 'NN'), (';', ':'), ('and', 'CC'), ('had', 'VBD'), ('lived', 'VBN'), ('nearly', 'RB'), ('twenty-one', 'CD'), ('years', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('world', 'NN'), ('with', 'IN'), ('very', 'RB'), ('little', 'JJ'), ('to', 'TO'), ('distress', 'VB'), ('or', 'CC'), ('vex', 'VB'), ('her', 'PRP'), ('.', '.')], [('She', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('youngest', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('two', 'CD'), ('daughters', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('most', 'RBS'), ('affectionate', 'JJ'), (',', ','), ('indulgent', 'JJ'), ('father', 'NN'), (';', ':'), ('and', 'CC'), ('had', 'VBD'), (',', ','), ('in', 'IN'), ('consequence', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('sister', 'NN'), (\"'s\", 'POS'), ('marriage', 'NN'), (',', ','), ('been', 'VBN'), ('mistress', 'NN'), ('of', 'IN'), ('his', 'PRP$'), ('house', 'NN'), ('from', 'IN'), ('a', 'DT'), ('very', 'RB'), ('early', 'JJ'), ('period', 'NN'), ('.', '.')], [('Her', 'PRP$'), ('mother', 'NN'), ('had', 'VBD'), ('died', 'VBN'), ('too', 'RB'), ('long', 'RB'), ('ago', 'RB'), ('for', 'IN'), ('her', 'PRP$'), ('to', 'TO'), ('have', 'VB'), ('more', 'JJR'), ('than', 'IN'), ('an', 'DT'), ('indistinct', 'JJ'), ('remembrance', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('caresses', 'NNS'), (';', ':'), ('and', 'CC'), ('her', 'PRP$'), ('place', 'NN'), ('had', 'VBD'), ('been', 'VBN'), ('supplied', 'VBN'), ('by', 'IN'), ('an', 'DT'), ('excellent', 'JJ'), ('woman', 'NN'), ('as', 'IN'), ('governess', 'NN'), (',', ','), ('who', 'WP'), ('had', 'VBD'), ('fallen', 'VBN'), ('little', 'JJ'), ('short', 'JJ'), ('of', 'IN'), ('a', 'DT'), ('mother', 'NN'), ('in', 'IN'), ('affection', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# run tagger\n",
    "taggedtext = [pos_tag(tokens) for tokens in sentlisttokens]\n",
    "print(taggedtext[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'NNS'), ('Emma', 'NNP'), ('by', 'IN'), ('Jane', 'NNP'), ('Austen', 'NNP'), ('1816', 'CD'), (']', 'NNP'), ('VOLUME', 'NNP'), ('I', 'PRP'), ('CHAPTER', 'VBP'), ('I', 'PRP'), ('Emma', 'NNP'), ('Woodhouse', 'NNP'), (',', ','), ('handsome', 'NN'), (',', ','), ('clever', 'NN'), (',', ','), ('and', 'CC'), ('rich', 'JJ'), (',', ','), ('with', 'IN'), ('a', 'DT'), ('comfortable', 'JJ'), ('home', 'NN'), ('and', 'CC'), ('happy', 'JJ'), ('disposition', 'NN'), (',', ','), ('seemed', 'VBD'), ('to', 'TO'), ('unite', 'VB'), ('some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('blessings', 'NNS'), ('of', 'IN'), ('existence', 'NN'), (';', ':'), ('and', 'CC'), ('had', 'VBD'), ('lived', 'VBN'), ('nearly', 'RB'), ('twenty-one', 'CD'), ('years', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('world', 'NN'), ('with', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "# flatten the list to get just one list of (word, tag)\n",
    "taggedtext_flat = [pair for sent in taggedtext for pair in sent]\n",
    "print(taggedtext_flat[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Frequent Tags in Emma:\n",
      "\n",
      "NN 19330\n",
      "IN 17880\n",
      "PRP 15619\n",
      "RB 12997\n",
      "DT 12743\n",
      ", 12016\n",
      "JJ 10249\n",
      "NNP 9095\n",
      "VBD 9049\n",
      "VB 8941\n"
     ]
    }
   ],
   "source": [
    "# Frequency Distribution of tags\n",
    "tags_fd = FreqDist(tag for (word, tag) in taggedtext_flat)\n",
    "# Top 10 tags\n",
    "print(\"Top 10 Most Frequent Tags in Emma:\\n\")\n",
    "for tag, freq in tags_fd.most_common(10):\n",
    "    print(tag, freq)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python36564bitbasecondacd90c988128e4e428ceb9fe9a1123e59"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
