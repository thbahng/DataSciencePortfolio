{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Lab Session Week 1\n",
    "# Word Frequencies and Frequency Distributions from text in NLTK\n",
    "\n",
    "### Demonstration of using nltk's word_tokenize method, FreqDist class and ngrams function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Processing Text using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Text from the NLTK Gutenberg corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view some books obtained from the Gutenberg online book project:\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this lab, we'll work with the first book, Jane Austen's \"Emma\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'austen-emma.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get ID of book we want\n",
    "file0 = nltk.corpus.gutenberg.fileids()[0]\n",
    "file0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "887071"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get original text as a string of characters given file ID\n",
    "emmatext = nltk.corpus.gutenberg.raw(file0)\n",
    "len(emmatext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(emmatext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text contains 887071 characters. Next we can view the first 120 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nan'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emmatext[:120] # first 120 characters in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Text\n",
    "\n",
    "NLTK has several tokenizers available to break the raw text into tokens; we will use one trained on news articles that separates by white space and by special characters (punctuation), but also leaves together some of these such as Mr.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191776"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get tokens from text\n",
    "emmatokens = nltk.word_tokenize(emmatext)\n",
    "len(emmatokens) # count tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text contains 191785 tokens that were either separated by white space or punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with']\n"
     ]
    }
   ],
   "source": [
    "# view first 50 tokens\n",
    "print(emmatokens[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably want to use the lowercase versions of the words.  To do this, we can use the function “lower()” that reduces all characters in a string to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with']\n",
      "191776\n"
     ]
    }
   ],
   "source": [
    "emmawords = [w.lower() for w in emmatokens] # convert tokens to lower\n",
    "print(emmawords[:50]) # first 50 lower tokens\n",
    "print(len(emmawords)) # count of lower tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function “set” converts the list of words to a set of words (which are only the unique ones), then the function “sorted” puts them into alphabetical order. This is called a **vocabulary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '&', \"'\", \"''\", \"'d\", \"'s\", \"'t\", \"'ye\", '(', ')', ',', '--', '.', '10,000', '1816', '23rd', '24th', '26th', '28th', '7th', '8th', ':', ';', '?', '[', ']', '_______', '_a_', '_accepted_', '_adair_', '_addition_', '_all_', '_almost_', '_alone_', '_amor_', '_and_', '_answer_', '_any_', '_appropriation_', '_as_', '_assistance_', '_at_', '_bath_', '_be_', '_been_', '_blunder_', '_boiled_', '_both_', '_bride_', '_broke_']\n"
     ]
    }
   ],
   "source": [
    "emmavocab = sorted(set(emmawords)) # sorted list of unique tokens (i.e. vocabulary)\n",
    "print(emmavocab[:50]) # first 50 of sorted list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngrams\n",
    "\n",
    "After word tokenization, you can extract bigrams (two-word phrases) and trigrams (three-word phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((',', 'and'), 1880), (('.', \"''\"), 1158), ((\"''\", '``'), 958), ((';', 'and'), 867), (('to', 'be'), 593), ((',', \"''\"), 584), (('.', 'I'), 570), ((',', 'I'), 569), (('of', 'the'), 556), (('in', 'the'), 434), ((';', 'but'), 427), (('.', '``'), 415), (('.', 'She'), 413), (('I', 'am'), 394), ((',', 'that'), 360), (('!', '--'), 344), (('had', 'been'), 307), ((',', 'she'), 304), ((',', 'but'), 303), (('.', 'He'), 303), (('--', 'and'), 302), ((',', 'as'), 292), (('it', 'was'), 282), (('I', 'have'), 281), (('could', 'not'), 277), (('Mr.', 'Knightley'), 273), (('.', 'It'), 266), ((\"''\", 'said'), 265), ((',', 'to'), 264), (('``', 'I'), 261)]\n",
      "<class 'zip'>\n"
     ]
    }
   ],
   "source": [
    "# count the frequency by bigram\n",
    "bigrams = ngrams(emmatokens,2)\n",
    "print(Counter(bigrams).most_common(30))\n",
    "print(type(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('.', \"''\", '``'), 757), ((',', \"''\", 'said'), 225), (('?', \"''\", '``'), 147), ((\"''\", '``', 'I'), 136), (('I', 'do', 'not'), 135), (('.', 'It', 'was'), 117), (('I', 'am', 'sure'), 107), ((',', 'however', ','), 89), ((',', 'and', 'the'), 89), ((',', 'my', 'dear'), 87), (('Miss', 'Woodhouse', ','), 86), (('.', 'She', 'was'), 85), (('.', \"''\", 'Emma'), 79), (('.', '``', 'I'), 78), ((',', 'I', 'am'), 78), (('``', 'Oh', '!'), 76), (('.', 'I', 'am'), 75), (('Mr.', 'Knightley', ','), 75), (('Mrs.', 'Weston', ','), 73), ((\"''\", '``', 'Oh'), 71), ((',', 'you', 'know'), 70), (('I', 'can', 'not'), 66), (('a', 'great', 'deal'), 63), (('.', 'She', 'had'), 61), ((';', 'and', 'the'), 60), (('would', 'have', 'been'), 59), (('.', 'He', 'had'), 58), (('.', 'It', 'is'), 58), ((\"''\", 'said', 'he'), 58), ((',', 'it', 'was'), 57)]\n"
     ]
    }
   ],
   "source": [
    "# count the frequency by trigram\n",
    "trigrams = ngrams(emmatokens,3)\n",
    "print(Counter(trigrams).most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adhoc example\n",
    "sentence = \"I went to the park today and played with my son.\"\n",
    "tokens = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'went', 'to', 'the', 'park', 'today')\n",
      "('went', 'to', 'the', 'park', 'today', 'and')\n",
      "('to', 'the', 'park', 'today', 'and', 'played')\n",
      "('the', 'park', 'today', 'and', 'played', 'with')\n",
      "('park', 'today', 'and', 'played', 'with', 'my')\n",
      "('today', 'and', 'played', 'with', 'my', 'son')\n",
      "('and', 'played', 'with', 'my', 'son', '.')\n"
     ]
    }
   ],
   "source": [
    "n = 6\n",
    "sixgrams = ngrams(tokens, n)\n",
    "for grams in sixgrams:\n",
    "  print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a side note, regular expressions can help us get rid of special characters...\n",
    "\n",
    "## Find the counts of different words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"food\" appears 3 times in the text\n",
      "The word \"love\" appears 117 times in the text\n",
      "The word \"she\" appears 2336 times in the text\n",
      "The word \"the\" appears 5201 times in the text\n"
     ]
    }
   ],
   "source": [
    "# get specific word frequencies\n",
    "print('The word \"food\" appears {:d} times in the text'.format(emmawords.count('food')))\n",
    "print('The word \"love\" appears {:d} times in the text'.format(emmawords.count('love')))\n",
    "print('The word \"she\" appears {:d} times in the text'.format(emmawords.count('she')))\n",
    "print('The word \"the\" appears {:d} times in the text'.format(emmawords.count('the')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Word Frequency Distributions\n",
    "## Using NLTK Frequency Distributions to Count Words\n",
    "\n",
    "We will create a dictionary that has a list of (key, value) pairs where the key is the word and the value is the frequency, or the count of the number of occurrences of each word.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter', 'woodhouse', ',', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'existence', ';', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'world', 'very', 'little', 'distress', 'or', 'vex', 'her', '.', 'she', 'was', 'youngest', 'two']\n"
     ]
    }
   ],
   "source": [
    "fdist = FreqDist(emmawords) # get frequencies of all words; this is a dictionary\n",
    "fdistkeys = list(fdist.keys()) # get all keys in dictionary\n",
    "print(fdistkeys[:50]) # print first 50 keys in dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can treat the frequency distribution just like a Python dictionary and we can look at the frequencies of individual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"emma\" appears 855 times in the text\n",
      "The word \"the\" appears 5201 times in the text\n"
     ]
    }
   ],
   "source": [
    "# print the frequencies of specific words\n",
    "print('The word \"emma\" appears {:d} times in the text'.format(fdist['emma']))\n",
    "print('The word \"the\" appears {:d} times in the text'.format(fdist['the']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FreqDist module has a function to return the most frequent words in the corpus.  It is a list of (word, frequency) pairs, and we can use this list to print the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(',', 12016)\n",
      "('.', 6346)\n",
      "('the', 5201)\n",
      "('to', 5181)\n",
      "('and', 4877)\n",
      "('of', 4284)\n",
      "('i', 3177)\n",
      "('a', 3124)\n",
      "('--', 3100)\n",
      "('it', 2503)\n"
     ]
    }
   ],
   "source": [
    "topkeys = fdist.most_common(10) # 50 most common words (or punctuations)\n",
    "for pair in topkeys:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequencies listed here are the counts of words in the document, but sometimes what we want are normalized frequencies, for example, if we want to compare frequencies between 2 or more documents.  A normalized frequency (also sometimes confusingly just called frequency) is the count of the words occurring in the corpus divided by the total number of words in the document.  Here is one way to normalize our top frequency words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(',', 0.06265643250458869)\n",
      "('.', 0.03309068913732688)\n",
      "('the', 0.02712018187885867)\n",
      "('to', 0.02701589354246621)\n",
      "('and', 0.025430710829300852)\n",
      "('of', 0.022338561655264474)\n",
      "('i', 0.01656620223594193)\n",
      "('a', 0.01628983814450192)\n",
      "('--', 0.01616469214083097)\n",
      "('it', 0.013051685299516102)\n"
     ]
    }
   ],
   "source": [
    "# total number of words in document\n",
    "numwords = len(emmawords)\n",
    "# divide each frequency by the total number of words\n",
    "topkeysnormalized = [(word, freq/numwords) for word, freq in topkeys]\n",
    "for pair in topkeysnormalized:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", appears in 6.3 percent of the text\n",
      ". appears in 3.3 percent of the text\n",
      "the appears in 2.7 percent of the text\n",
      "to appears in 2.7 percent of the text\n",
      "and appears in 2.5 percent of the text\n",
      "of appears in 2.2 percent of the text\n",
      "i appears in 1.7 percent of the text\n",
      "a appears in 1.6 percent of the text\n",
      "-- appears in 1.6 percent of the text\n",
      "it appears in 1.3 percent of the text\n"
     ]
    }
   ],
   "source": [
    "for word, val in topkeysnormalized:\n",
    "    print(word + ' appears in {:.1f} percent of the text'.format(round(val*100,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick a different file number for the Gutenberg books, change the variable fileno to this number and rerun the steps to create a top 30 frequency distribution, FreqDist, for this book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blake-poems.txt\n",
      "The total number of characters in blake-poems.txt is 38153\n"
     ]
    }
   ],
   "source": [
    "#nltk.corpus.gutenberg.fileids()\n",
    "file4 = nltk.corpus.gutenberg.fileids()[4] # specify file id we want\n",
    "print(file4)\n",
    "# get text for that file\n",
    "myText = nltk.corpus.gutenberg.raw(file4)\n",
    "print('The total number of characters in {:s} is {:d}'.format(file4, len(myText)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'poems', 'by', 'william', 'blake', '1789', ']', 'songs', 'of', 'innocence']\n"
     ]
    }
   ],
   "source": [
    "# tokenize text\n",
    "myTokens = nltk.word_tokenize(myText)\n",
    "# convert to lower\n",
    "myTokens = [w.lower() for w in myTokens]\n",
    "print(myTokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 685), ('the', 439), ('and', 348), ('.', 221), ('of', 146), ('in', 141), ('i', 130), ('a', 127), ('to', 111), (';', 99), ('my', 83), (':', 76), ('!', 68), ('with', 66), ('?', 65), ('his', 57), ('he', 56), ('is', 52), ('``', 50), (\"'s\", 48), ('little', 45), ('on', 44), ('they', 44), ('not', 43), ('thee', 42), ('that', 39), ('all', 39), ('but', 38), (\"''\", 35), ('like', 35)]\n"
     ]
    }
   ],
   "source": [
    "# frequency distribution of words in my tokens\n",
    "myFreqDist = FreqDist(myTokens)\n",
    "# top 30 most common key value pairs\n",
    "myTop30 = myFreqDist.most_common(30)\n",
    "print(myTop30)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
